My name is Juliean Rajakanthan and I am a BSc Mathematics graduate with a strong interest in using data to solve real problems, inform decisions and create meaningful impact. I combine a solid academic foundation in analytical thinking with practical experience in technology and a recently completed data bootcamp, making me well-prepared to begin my career as a data analyst.

My background includes working at Accenture as an Application Development Analyst, where I gained exposure to a wide range of disciplines including DevOps, IT security and generative AI. I contributed to several impactful projects such as building a server monitoring dashboard that improved stability and reduced manual reporting time by 40 percent. I also worked as a Prompt Engineer on a generative AI initiative, using OpenAI APIs including Whisper to automate meeting transcription and support the development of internal AI tools for tax advisory use cases.

In addition to technical responsibilities, I served as the communications lead for my team’s pillar. I played a key role in co-hosting internal events on cybersecurity awareness, educating colleagues on phishing threats and promoting best practices. This work was recognised with a Silver award for contributions to IT security.

To build on this experience and sharpen my analytical skills, I completed a full-time data bootcamp with Just IT. Through this programme, I developed practical expertise across the data lifecycle. I worked with real-world datasets to clean, transform, analyse and present data in a way that supports clear decision-making. I became confident using tools such as Python, SQL, Power BI, Tableau, Excel and Azure. I also practised delivering insights to non-technical audiences with clarity and impact, something I enjoy and consider a key strength.

I am now focused on pursuing a role as a data analyst where I can apply this combination of mathematical reasoning, technical skills and business awareness. I am particularly excited about opportunities to work with diverse datasets, identify patterns and trends, and deliver recommendations that support evidence-based decisions. I value collaboration, learning and continuous improvement, and I am keen to contribute to a forward-thinking team where I can grow and make a difference through data.



Sunshine Hours and Consumer Engagement Analysis
Overview
This repository contains the code, data processing scripts, and visualisations used to explore the relationship between sunshine hours and consumer engagement. The project investigates how variations in weather, specifically sunshine duration, impact consumer mood and comfort, and consequently influence their engagement behaviours such as time spent on site, page views, and interaction rates.

Understanding these connections can help businesses and content creators optimise the timing and nature of their offerings to align with environmental factors, potentially increasing user satisfaction and engagement.

Project Objectives
The primary objectives of this project are to:

Analyse historical weather data focusing on sunshine hours.

Examine consumer engagement metrics and their variations with weather conditions.

Identify correlations or trends that suggest weather-driven mood and comfort influence engagement.

Provide actionable insights that can inform content strategy and marketing efforts.

Data Description
The project utilises one main data sources:

Weather Data: Includes daily sunshine hours and other meteorological variables such as temperature, precipitation, and cloud cover, covering the period [1979 to 2020].


Methodology
The analysis follows a structured approach:

Data Cleaning and Preprocessing: Handling missing values, removing outliers, and aligning datasets by date.

Feature Engineering: Creating composite indices such as a Comfort Index based on weather variables to better represent user comfort.

Exploratory Data Analysis (EDA): Visualising trends, seasonal patterns, and initial correlations between sunshine hours and engagement.

Statistical Analysis: Applying correlation tests and regression models to quantify relationships and test significance.

Segmentation: Clustering days based on engagement and weather profiles to identify patterns.

Reporting: Summarising findings and producing visual outputs to support conclusions.

Repository Structure
notebooks/ – Contains Jupyter notebooks used for data cleaning, analysis, and visualisation.

data/ – Holds raw and processed datasets (note: sensitive data excluded or anonymised).


visualisations/ – Stores figures, charts, and graphs generated during analysis.
